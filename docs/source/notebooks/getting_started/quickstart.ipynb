{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc89db3",
   "metadata": {},
   "source": [
    "# Quickstart: scalar differentiation\n",
    "\n",
    "Finite difference methods estimate derivatives of functions from point-evaluations of said function.\n",
    "The same is true for probabilistic finite differences.\n",
    "\n",
    "This set of notes explains the very basics of computing numerical derivatives with `probfindiff`. As a side quest, some basic design choices are explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57dd1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "from probfindiff import central, differentiate, forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63268d",
   "metadata": {},
   "source": [
    "## First-order derivatives\n",
    "\n",
    "At the heart of `probfindiff`, there is the function `differentiate()`, and a set of finite difference schemes.\n",
    "For example, to differentiate a function with a central scheme, compute the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21df53cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0003637 1.0\n"
     ]
    }
   ],
   "source": [
    "scheme, xs = central(dx=0.01)\n",
    "dfx, _ = differentiate(jnp.sin(xs), scheme=scheme)\n",
    "\n",
    "print(dfx, jnp.cos(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a8056",
   "metadata": {},
   "source": [
    "The function `differentiate` acts on point-evaluations of a function on some grid-points.\n",
    "These points can be chosen by a user, but more often than not, they are coupled tightly to the scheme itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d88466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs = [-0.01  0.    0.01]\n",
      "\n",
      "scheme = FiniteDifferenceScheme(weights=DeviceArray([-5.0015533e+01, -6.9692903e-03,  5.0022503e+01], dtype=float32), covs_marginal=DeviceArray(-0.00038028, dtype=float32), order_derivative=DeviceArray(1, dtype=int32, weak_type=True))\n"
     ]
    }
   ],
   "source": [
    "print(\"xs =\", xs)\n",
    "print()\n",
    "print(\"scheme =\", scheme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b2f47a",
   "metadata": {},
   "source": [
    "The function ``differentiate()`` is self is so simple and lightweight, you could in fact implement it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fba5573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0003637 1.0\n"
     ]
    }
   ],
   "source": [
    "dfx = jnp.sin(xs) @ scheme.weights\n",
    "print(dfx, jnp.cos(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f63ab2",
   "metadata": {},
   "source": [
    "The finite difference scheme expects that the array consists of function evaluations at a specific grid.\n",
    "This is important, because, for instance, smaller step-sizes imply different weights/coefficients, and different accuracy.\n",
    "\n",
    "\n",
    "The requirement of acting only on discretised functions is different to many existing finite difference implementations, which behave more like automatic differentiation (i.e., they act on the function _as a function_ and evaluate it internally).\n",
    "\n",
    "\n",
    "**Why?** This design choice is deliberate. In many applications, e.g. differential equations, the number of function evaluations counts. Depending on the implementation, some functions can also be batched efficiently, while others cannot.\n",
    "To make this transparent, `probfindiff` lets a user evaluate their functions themselves.\n",
    "It is therefore closer to `np.gradient` than to automatic differentiation.\n",
    "(There are also some other advantages regarding types, compilation, and vectorisation, but this is left for a different tutorial.)\n",
    "\n",
    "\n",
    "## Higher-order derivatives\n",
    "\n",
    "It is easy to compute higher-order derivatives by changing the scheme accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42bd8d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1569691e-06 -0.0\n"
     ]
    }
   ],
   "source": [
    "scheme, xs = central(dx=0.01, order_derivative=2)\n",
    "d2fx, _ = differentiate(jnp.sin(xs), scheme=scheme)\n",
    "\n",
    "print(d2fx, -jnp.sin(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ea274",
   "metadata": {},
   "source": [
    "## Higher-order methods\n",
    "\n",
    "To increase the accuracy of the approximation, the method-order can be increased freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9b3074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998326 1.0\n"
     ]
    }
   ],
   "source": [
    "scheme, xs = central(dx=0.02, order_method=4)\n",
    "dfx, _ = differentiate(jnp.sin(xs), scheme=scheme)\n",
    "\n",
    "print(dfx, jnp.cos(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5346f1",
   "metadata": {},
   "source": [
    "## Forward, central, and backward schemes\n",
    "\n",
    "While central schemes tend to be more accurate than forward and backward schemes, all three are available. For example, we can replace the central scheme with a forward scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bbfb1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0013572 1.0\n"
     ]
    }
   ],
   "source": [
    "scheme, xs = forward(dx=0.02)\n",
    "dfx, _ = differentiate(jnp.sin(xs), scheme=scheme)\n",
    "\n",
    "print(dfx, jnp.cos(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6530a",
   "metadata": {},
   "source": [
    "## What has been left out?\n",
    "\n",
    "In all the examples above, we have ignored the second output of `differentiate()`: the uncertainty associated with the estimate.\n",
    "Its meaning, and how to make the most of it, are subject for a different tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
