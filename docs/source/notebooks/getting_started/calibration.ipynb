{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a589a1a",
   "metadata": {},
   "source": [
    "# Calibration and model selection\n",
    "\n",
    "Probabilistic numerical finite differences use the formalism of Gaussian process regression to derive the schemes.\n",
    "This brings with it the advantage of uncertainty quantification, but also the burden of choosing a useful prior model.\n",
    "\n",
    "In this notebook, we will discuss the very basics of model selection and uncertainty quantification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74124e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.stats\n",
    "\n",
    "import probfindiff\n",
    "from probfindiff.utils import kernel, kernel_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9911aa",
   "metadata": {},
   "source": [
    "First, a baseline. With a bad scale-parameter-estimate,, the error and uncertainty quantification are off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d737cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale:\n",
      "\t 100.0\n",
      "Error:\n",
      "\t 0.67733574\n",
      "Standard deviation:\n",
      "\t 3.8611143\n"
     ]
    }
   ],
   "source": [
    "def k(*, input_scale):\n",
    "    \"\"\"Fix the input scale of an exponentiated quadratic kernel.\"\"\"\n",
    "    return functools.partial(\n",
    "        kernel_zoo.exponentiated_quadratic, input_scale=input_scale\n",
    "    )\n",
    "\n",
    "\n",
    "dx = 0.1\n",
    "\n",
    "# an incorrect scale messes up the result\n",
    "scale = 100.0\n",
    "scheme, xs = probfindiff.central(dx=dx, kernel=k(input_scale=scale))\n",
    "\n",
    "f = lambda x: jnp.cos((x - 1.0) ** 2)\n",
    "fx = f(xs)\n",
    "dfx, variance = probfindiff.differentiate(fx, scheme=scheme)\n",
    "\n",
    "dfx_true = jax.grad(f)(0.0)\n",
    "error, std = jnp.abs(dfx - dfx_true), jnp.sqrt(variance)\n",
    "print(\"Scale:\\n\\t\", scale)\n",
    "print(\"Error:\\n\\t\", error)\n",
    "print(\"Standard deviation:\\n\\t\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77736414",
   "metadata": {},
   "source": [
    "We can tune the prior kernel to alleviate this issue.\n",
    "For example, we can compute the maximum-likelihood estimate of the input-scale $\\theta$.\n",
    "The goal is to find\n",
    "$$\n",
    "\\arg\\max_{\\theta} p(f_{\\theta}(x_n) = f_n, ~ n=0, ..., N \\mid \\theta)\n",
    "$$\n",
    "where $f_\\theta$ is the prior Gaussian process, $f_n$ are the observations of the to-be-differentiated function, and $x_n$ are the finite difference grid points.\n",
    "\n",
    "The problem is small, so let us be lazy and compute the minimum with a grid-search over a logarithmic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaa8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=(\"kernel_from_scale\",))\n",
    "def mle_input_scale(*, xs_data, fx_data, kernel_from_scale, input_scale_trials):\n",
    "    \"\"\"Compute the maximum-likelihood-estimate for the input scale.\"\"\"\n",
    "\n",
    "    # Fix all non-varying parameters, vectorise, and JIT.\n",
    "    scale_to_logpdf = functools.partial(\n",
    "        input_scale_to_logpdf,\n",
    "        fx_data=fx_data,\n",
    "        kernel_from_scale=kernel_from_scale,\n",
    "        xs_data=xs_data,\n",
    "    )\n",
    "    scale_to_logpdf_optimised = jax.jit(jax.vmap(scale_to_logpdf))\n",
    "\n",
    "    # Compute all logpdf values for some trial inputs.\n",
    "    logpdf_values = scale_to_logpdf_optimised(input_scale=input_scale_trials)\n",
    "\n",
    "    # Truly terrible input scales lead to NaN values.\n",
    "    # They are obviously not good candidates for the optimum.\n",
    "    logpdf_values_filtered = jnp.nan_to_num(logpdf_values, -jnp.inf)\n",
    "\n",
    "    # Select the optimum\n",
    "    index_max = jnp.argmax(logpdf_values_filtered)\n",
    "    return input_scale_trials[index_max]\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=(\"kernel_from_scale\",))\n",
    "def input_scale_to_logpdf(*, input_scale, xs_data, fx_data, kernel_from_scale):\n",
    "    \"\"\"Compute the logpdf of some data given an input-scale.\"\"\"\n",
    "\n",
    "    # Select a kernel with the correct input-scale\n",
    "    k_scale = kernel_from_scale(input_scale=input_scale)\n",
    "    k_batch = kernel.batch_gram(k_scale)[0]\n",
    "\n",
    "    # Compute the Gram matrix and evaluate the logpdf\n",
    "    K = k_batch(xs_data, xs_data.T)\n",
    "    return jax.scipy.stats.multivariate_normal.logpdf(\n",
    "        fx_data, mean=jnp.zeros_like(fx_data), cov=K\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5e6860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimised input scale is:\n",
      "\ts = 2.3462286\n"
     ]
    }
   ],
   "source": [
    "scale = mle_input_scale(\n",
    "    xs_data=xs,\n",
    "    fx_data=fx,\n",
    "    kernel_from_scale=k,\n",
    "    input_scale_trials=jnp.logspace(-3, 4, num=1_000, endpoint=True),\n",
    ")\n",
    "print(\"The optimised input scale is:\\n\\ts =\", scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ffb7a",
   "metadata": {},
   "source": [
    "The resulting parameter estimate improves the calibration significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1910ba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale:\n",
      "\t 2.3462286\n",
      "Error:\n",
      "\t 0.019150138\n",
      "Standard deviation:\n",
      "\t 0.0146484375\n"
     ]
    }
   ],
   "source": [
    "scheme, xs = probfindiff.central(dx=dx, kernel=k(input_scale=scale))\n",
    "dfx, variance = probfindiff.differentiate(f(xs), scheme=scheme)\n",
    "\n",
    "error, std = jnp.abs(dfx - dfx_true), jnp.sqrt(variance)\n",
    "print(\"Scale:\\n\\t\", scale)\n",
    "print(\"Error:\\n\\t\", error)\n",
    "print(\"Standard deviation:\\n\\t\", std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
